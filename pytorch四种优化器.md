<center><h1>四种优化器</h1></center>

### 随机梯度下降 SGD Stochastic Gradient Descent

当训练数据十分巨大的时候，计算整体的cost function来求梯度代价很大，时间和存储都会有所影响，比较常用的一个方法就是使用样本中的小批量样本来近似整体样本，使用小批量样本进行参数更新，这就是批量梯度的基本思想，SGD算是批量梯度的一种特殊方法

### 原理

在每次更新时使用一个样本来近似所有样本，该样本时从训练集中随机选取，这样相比批量梯度而言，计算速度会进一步提升，但是这样也会带来缺点，样本数选的过少会导致计算出来的梯度一定程度上与实际梯度有所偏差，因此每次优化的方向不会是最优方向，但是整体方向是朝着正确方向前进。

#### 公式



